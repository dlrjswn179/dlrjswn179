# -*- coding: utf-8 -*-
"""비교과 노트.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kedm25P6dk0oNsC34w0XsETaNtDb2wJE

#1주차

**기초**
"""

print("Hello World")

a = 123

a

type(a)

a = "123"

a

type(a)

#List
a = [1,2,3]
b = ["1","2","3"]
c = [1,"1",a]

#Tuple(수정불가능)
d = (1,2,3)
e = "1","2","3"
f = (1,"1",a)

#Set(순서없음)
g = {1,2,3}

#Dictionary
h = {'a':1,'b':2,'c':3}

c[2]

e

g[2]
#Set은 순서가 없기 때문

h[0]

h[a]

h['a']

1+1

2-1

1*1

2/1

2>1

2<1

2>=1

2<=1

2==1

2!=1

1 in a

1 in d

1 in g

1 in e

"""**데이터 분석에 적용**

"""

#데이터 업로드
import os
os.getcwd()

import pandas as pd

bike_data = pd.read_csv('bike_usage_0.csv',encoding='cp949')
#원래는 ansi를 utf-8으로.

bike_data

print(bike_data)

bike_data.isnull()

bike_data.isnull().sum()

population = pd.read_csv('population_by_Gu.txt')

population

population = pd.read_csv('population_by_Gu.txt',sep = '\t')
#tap을 기준으로 seperate시킴.

population

weather = pd.read_csv('weather.csv')

weather

bike_data.describe()
#평균 등 간단한 값 설명

type(bike_data)

bike_data.columns

bike_data.head()

bike_data.tail()

bike_data.shape

bike_data.info()

bike_data.sum()

bike_data.Distance

bike_data['Distance']

bike_data['Distance'].sum()

bike_data['Membership_type'].unique()

bike_data['Membership_type'].value_counts()

bike_data['Membership_type'].value_counts(normalize = 'True')

bike_data.info()

bike_data.Momentum.isnull().sum()

bike_data.Momentum.min()

bike_data.Momentum.max()

bike_data[bike_data.Momentum=='\\N']

"""**데이터** **정제** (정제, 통합, 축소, 변환)

"""

bike_data[bike_data.Gender.isnull()]

bike_data.loc[bike_data.Gender.isnull(),'Gender'] = 'U'

bike_data[bike_data.Gender.isnull()]

bike_data.Gender.value_counts()

bike_data[bike_data.Momentum == '\\N']

import numpy as np

bike_data.loc[bike_data.Momentum=='\\N','Momentum'] = np.nan

bike_data[bike_data.Momentum == '\\N']

bike_data.dropna()

bike_data.info()

bike_data.dropna(inplace = True)

bike_data.info()

bike_data[['Momentum','Carbon_amount']].dtypes

bike_data[['Momentum','Carbon_amount']] = bike_data[['Momentum','Carbon_amount']].astype(float)

bike_data[['Momentum','Carbon_amount']].dtypes

bike_data.info()

#이상값. Tukey 함수 구현!
import numpy as np
def outliers_iqr(data):
  q1,q3 = np.percentile(data,[25,75])
  iqr = q3 - q1
  lower_bound = q1 - (iqr*1.5)
  upper_bound = q3 + (iqr*1.5)
  return np.where((data > upper_bound) | (data < lower_bound))

outliers = outliers_iqr(bike_data.Distance)

np.size(outliers)

outliers

bike_data.iloc[outliers]

"""**데이터 결합**"""

bike_data.info()

bike_data

stations = pd.read_csv('stations.csv')

stations

bike_data2 = pd.merge(bike_data, stations, left_on = 'Station_no_out', right_on = 'ID')

bike_data.info()

#데이터 통합 연습하기
df1 = pd.DataFrame({'a':['a','b','c'], 'b':[1,2,3]})
df2 = pd.DataFrame({'a':['a','b','d'], 'c':['가','나','다']})

df1

df2

pd.merge(df1, df2, how = 'inner', on='a')

pd.merge(df1, df2, how = 'outer', on='a')

pd.merge(df1, df2, how = 'left', on='a')

pd.merge(df1, df2, how = 'right', on='a')

#이로써 데이터 준비 끝!

"""#2주차

**시각화**
"""

import matplotlib.pyplot as plt

#히스토그램
plt.hist(bike_data2.Distance, color= 'blue')
plt.show()

plt.hist(bike_data2.Distance, color= 'blue', bins=1000)
plt.show()

#boxplot
plt.boxplot(bike_data2.Distance)
plt.show()

under_5000= bike_data2[bike_data2.Distance<5000]
plt.boxplot(under_5000.Distance)
plt.show()

under_5000= bike_data2[bike_data2.Distance<5000]
plt.boxplot([under_5000.Distance[under_5000.Gender=='F'],under_5000.Distance[under_5000.Gender=='M']])
plt.xticks([1,2],['Female','Male'])
plt.show()

#선그래프
plt.plot(bike_data['Distance'].groupby(bike_data['Date_out']).sum())
plt.show()

#바그래프
plt.bar(labels,height=sizes,color='blue')
plt.show()

bike_data2['Gender'].value_counts().plot(kind='bar')
plt.show()

"""**데이터 전처리**"""

bike_data2.Gender.value_counts()

bike_data2[bike_data2.Gender=='f']

bike_data2.loc[bike_data2.Gender=='f','Gender']
#우리는 F가 문제없음

bike_data2[bike_data2.Gender=='m']='M'

bike_data2.Gender.value_counts()

#duration
bike_data2[bike_data2.Distance==0]

bike_data2.loc[bike_data2.Distance==0,'Duration'].max()

bike_data2=bike_data2[bike_data2.Distance!=0]

bike_data2[bike_data2.Duration==0]

#데이터 요약
pd.pivot_table(bike_data2, index='Age_Group', columns='Membership_type', values='Distance', aggfunc=np.sum)

bike_pivot= pd.pivot_table(bike_data2, index='Age_Group', columns='Membership_type', values='Distance', aggfunc=np.sum)

bike_pivot

bike_pivot= bike_pivot.reset_index()

pd.melt(bike_pivot, id_vars='Age_Group', value_vars=['단체권','일일권','정기권'],var_name='Membership_type',value_name='Total_Dist')

"""**정규분포**"""

import matplotlib.pyplot as plt
import numpy as np
#히스토그램
random_sample = np.random.normal(loc=10,scale=2,size=1000)
plt.hist(random_sample,bins=100)
plt.show()

#QQPlot
import numpy as np
import pylab
import scipy.stats as stats

norm_sample = np.random.normal(loc=20, scale=5, size=100)
stats.probplot(norm_sample, dist='norm', plot=pylab)
pylab.show()

import random
avg_values = []
for i in range (1,11):
  random_sample = random.sample(range(1,1000),100)
  x=np.mean(random_sample)
  avg_values.append(x)

plt.hist(avg_values, bins=100)
plt.show()

import random
avg_values = []
for i in range (1,101):
  random_sample = random.sample(range(1,1000),100)
  x=np.mean(random_sample)
  avg_values.append(x)

plt.hist(avg_values, bins=100)
plt.show()

import random
avg_values = []
for i in range (1,1001):
  random_sample = random.sample(range(1,1000),100)
  x=np.mean(random_sample)
  avg_values.append(x)

plt.hist(avg_values, bins=100)
plt.show()

import random
avg_values = []
for i in range (1,10001):
  random_sample = random.sample(range(1,1000),100)
  x=np.mean(random_sample)
  avg_values.append(x)

plt.hist(avg_values, bins=100)
plt.show()

"""**가설검정과 t-검정**"""

bike_data2.Gu.value_counts()

y_gu=bike_data2[bike_data2.Gu=='영등포구']
m_gu=bike_data2[bike_data2.Gu=='마포구']

from scipy import stats
stats.levene[y_gu.Distance, m_gu.Distance]

np.mean(y_gu.Distance)

np.mean(m_gu.Distance)

stats.ttest_ind(y_gu.Distance, m_gu.Distance, equal_var=True)
#등분산이라고 판정했으므로 True라 설정.

s_gu=bike_data2[bike_data2.Gu=='서초구']
d_gu=bike_data2[bike_data2.Gu=='동대문구']
e_gu=bike_data2[bike_data2.Gu=='은평구']

from scipy import stats
stats.bartlett(y_gu.Distance, m_gu.Distance, s_gu.Distance,d_gu.Distance,e_gu.Distance )

stats.f_oneway(y_gu.Distance,m_gu.Distance, s_gu.Distance,d_gu.Distance,e_gu.Distance)

plot_data=[y_gu.Distance, m_gu.Distance, s_gu.Distance,d_gu.Distance,e_gu.Distance]
plt.boxplot(plot_data)
plt.show()

from statsmodels.stats.multicomp import pairwise_tukeyhsd
hsd= pairwise_tukeyhsd(bike_data2.Distance,bike_data2.Gu)
hsd.summary()

"""**카이제곱검정**"""

from scipy.stats import chi2_contingency
crosstab=pd.crosstab(bike_data2.Age_Group,bike_data2.Membership_type)
chi2_contingency(crosstab)

result=chi2_contingency(crosstab)
print('Chi2 Statistic : {}, p-value:{}'.format(result[0],result[1]))

"""#3주차

**상관분석**
"""

dist_by_gu = pd.pivot_table(bike_data2, index='Gu', values='Distance', aggfunc=len)
dist_by_gu

by_gu = pd.merge(dist_by_gu, population, on='Gu')
by_gu

import matplotlib.pyplot as plt
plt.scatter(by_gu.Distance, by_gu.Population)
plt.show()

from scipy import stats
stats.pearsonr(by_gu.Distance, by_gu.Population)

by_gu = pd.merge(dist_by_gu, population, on='Gu')[['Gu','Distance','Population']]
by_gu.corr()

"""**회귀분석**"""

weather

bike_data2

new_weather = pd.pivot_table(weather, index=['date','time'],values=['temp','cum_precipitation','humidity','insolation','sunshine','wind','wind_direction','sea_lvl_pressure','pressure'],aggfunc=np.mean)
new_weather

new_weather.reset_index()

new_bike=pd.pivot_table(bike_data2, index=['Date_out','Time_out'], values=['Distance'], aggfunc=len)
new_bike=new_bike.reset_index()
new_bike

new_bike.rename(columns={'Distance':'Count'},inplace=True)
new_bike.columns

bike_weather=pd.merge(new_bike, new_weather, left_on=['Date_out','Time_out'], right_on=['date','time'])
bike_weather

stats.linregress(bike_weather.temp, bike_weather.Count)

slop, intercept, r_value, p_value, std_err = stats.linregress(bike_weather.temp, bike_weather.Count)
print("R-squared:%f"%r_value**2)

import statsmodels.api as sm
X0=bike_weather.temp
X1=sm.add_constant(X0)
y=bike_weather.Count
model=sm.OLS(y,X1)
result=model.fit()
print(result.summary())

"""**다중회귀분석**"""

bike_weather['Rain_YN']='N'
bike_weather.loc[bike_weather.cum_precipitation>0,'Rain_YN']='Y'
bike_weather

ohe = pd.get_dummies(bike_weather['Rain_YN'])
ohe

bike_weather = pd.concat([bike_weather, ohe],axis=1, sort= False)
bike_weather

from sklearn.model_selection import train_test_split

X=bike_weather[['humidity','temp','wind','N','Y']]
y=bike_weather.Count
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=123)

import statsmodels.api as sm

X1 = sm.add_constant(X_train)
model = sm.OLS(y_train, X1)
result = model.fit()
print(result.summary())

X1 = sm.add_constant(X_test)
pred = result.predict(X1)

from sklearn import metrics

print('MAE :', metrics.mean_absolute_error(y_test,pred))
print('MSE :', metrics.mean_squared_error(y_test,pred))
print('MAE :', np.sqrt(metrics.mean_absolute_error(y_test,pred)))
print('MAE :', np.mean(np.abs((y_test-pred)/y_test))*100)

"""**로지스틱 회귀분석**"""

bike_weather['over_500']=1
bike_weather.loc[bike_weather.Count<500,'over_500']=0

from sklearn.model_selection import train_test_split

X=bike_weather[['cum_precipitation','humidity','temp','wind']]
y=bike_weather.over_500
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=123)

import statsmodels.api as sm

X1 = sm.add_constant(X_train)
logit_model = sm.Logit(y_train, X1)
result = logit_model.fit()
print(result.summary())

from sklearn.linear_model import LogisticRegression

log_reg=LogisticRegression()
log_reg.fit(X_train, y_train)
print('Train set 정확도 : %.2f'%log_reg.score(X_train, y_train))
print('Test set 정확도 : %.2f'%log_reg.score(X_test, y_test))

from sklearn.metrics import classification_report

y_pred = log_reg.predict(X_test)
print(classification_report(y_test, y_pred))

"""**의사결정나무**"""

from sklearn import tree

X=X_train
y=y_train
dTree=tree.DecisionTreeClassifier()
dTreeModel=dTree.fit(X,y)
dTreeModel

from sklearn.tree import export_graphviz
import pydotplus
from IPython.display import Image

dot_data = export_graphviz(dTreeModel, out_file=None, feature_names=['cum_precipitation','humidity','temp','wind'], class_names=('Y','N'), filled=True, rounded=True, special_characters=True)
graph=pydotplus.graph_from_dot_data(dot_data)
Image(graph.create_png())

dTreeModel.predict(X_test)

from sklearn.metrics import accuracy_score
y_pred=dTreeModel.predict(X_test)
print('Accuracy : %.2f'%accuracy_score(y_test,y_pred))

y_pred=dTreeModel.predict(X_test)
print(classification_report(y_test,y_pred))

"""**머신러닝: 군집분석**"""

weather=pd.read_csv('weather.csv')

from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np

X=np.array(weather.humidity).reshape(-1,1)
scaler=MinMaxScaler()
X_scaled=scaler.fit_transform(X)
X_scaled

n_bike = pd.pivot_table(bike_data2, index=['Gu','Date_out','Time_out'], values='Distance',aggfunc=len)
n_bike = n_bike.reset_index()
n_bike.rename(columns={'Distance':'Count'}, inplace=True)
n_bike

n_bike2 = pd.pivot_table(n_bike, index='Gu', columns='Time_out', values='Count',aggfunc=np.mean)
n_bike2 = n_bike2.reset_index()
n_bike2

from sklearn import cluster
X=n_bike2.iloc[0:5, 1:25]
y=n_bike2.Gu
km2=cluster.KMeans(n_clusters = 2).fit(X)
km3=cluster.KMeans(n_clusters = 3).fit(X)
km4=cluster.KMeans(n_clusters = 4).fit(X)

n_bike2['2_Cluster']=km2.labels_
n_bike2['3_Cluster']=km3.labels_
n_bike2['4_Cluster']=km4.labels_
n_bike2[['Gu','2_Cluster','3_Cluster','4_Cluster']]

"""# 딥러닝을 활용한 부동산 가격 이상치 분석

데이터 업로드
"""

#데이터 업로드
import os
os.getcwd()
import pandas as pd
import numpy as np
data = pd.read_csv('DATASET_1.CSV')

"""데이터 탐색"""

#데이터 탐색
data

#결측값, 문자로 된 이상값 존재 확인
data.info()

#결측값 확인(2) (isnull().sum() 사용)
data.isnull().sum()
#결측값 없음!

#이상값(극단값) 확인
#극단값 판단법: "Tukey"법 사용(1사분위수 아래로 IQR*1.5 값보다 작거나, 3사분위수 위로 IQR*1.5값보다 큰 값)
#IQR==Q1에서 Q3까지의 값

print(data.shape)
#column의 개수 확인

#Tukey함수 구현.
def outliers_iqr(data, column):
  q1,q3 = np.quantile(data[column], 0.25) ,np.quantile(data[column], 0.75)
  iqr = q3 - q1
  lower_bound = q1 - (iqr*1.5)
  upper_bound = q3 + (iqr*1.5)
  data1 = data[data[column] > upper_bound]     
  data2 = data[data[column] < lower_bound]
  return print(n,'column의 극단값의 개수는', data1.shape[0] + data2.shape[0], '이다.')

for n in range(1,46):
  outliers_iqr(data, data.columns[n])
#이상값이 소수 존재하는 것을 확인. 정제할 필요가 있는지 확인해야함.

# 필요한 라이브러리 로드(시각화)
import matplotlib.pyplot as plt
from scipy import stats

#지역별 데이터 수를 시각화료 표현
labels = ["Others", "Seoul", "Gyeonggi"]
plt.xticks(range(3), labels)
plt.xlabel("Region")
plt.ylabel("The Number of Data")
data['Class'].value_counts().plot(kind = 'bar')

from sklearn.model_selection import train_test_split

del data["지역별"]
del data["시점"]

#정규화

data=(data - data.min()) / (data.max() - data.min())

data
